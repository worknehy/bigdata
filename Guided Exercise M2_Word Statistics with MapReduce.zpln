{"paragraphs":[{"text":"%md\n**1. Copy the data (stored in the local file system) to HDFS**\n*The collected works of Shakespeare, Austen, and Churchill are stored in a single text file each in the folder GE2. The text has been tokenized and stopword-filtered so that we can obtain accurate word statistics. \nUse the **ls** command:*\n\n\n","user":"anonymous","dateUpdated":"2025-10-26T20:42:05+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_1210838887","id":"paragraph_1761426621865_1950684783","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","focus":true,"dateFinished":"2025-10-26T20:42:05+0000","dateStarted":"2025-10-26T20:42:05+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>1. Copy the data (stored in the local file system) to HDFS</strong><br />\n<em>The collected works of Shakespeare, Austen, and Churchill are stored in a single text file each in the folder GE2. The text has been tokenized and stopword-filtered so that we can obtain accurate word statistics.<br />\nUse the <strong>ls</strong> command:</em></p>\n\n</div>"}]}},{"text":"%sh\n#verify that the files are present in the folder on your local filesystem using the ls command\nls GE2","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_1417860574","id":"paragraph_1761418104534_956546342","dateCreated":"2025-10-26T20:20:06+0000","status":"READY","focus":false},{"text":"%md\n*ls lists directory contents; by not specifying a path, it defaults to the current directory. You should see the files among the files and directories listed. Now we want to copy these files to HDFS. To interact with the Hadoop filesystem, which has been configured to use HDFS by default, we can use the hadoop fs command. Let's run it to see how we can interact with HDFS.\n\nYou can use **\"hadoop fs\"** or **\"hdfs dfs\"**, to be consistent in this exercise we will use \"hadoop fs\"","user":"anonymous","dateUpdated":"2025-10-26T20:42:09+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_1493830668","id":"paragraph_1761423158872_1709069126","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:42:09+0000","dateStarted":"2025-10-26T20:42:09+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>*ls lists directory contents; by not specifying a path, it defaults to the current directory. You should see the files among the files and directories listed. Now we want to copy these files to HDFS. To interact with the Hadoop filesystem, which has been configured to use HDFS by default, we can use the hadoop fs command. Let&rsquo;s run it to see how we can interact with HDFS.</p>\n<p>You can use <strong>&ldquo;hadoop fs&rdquo;</strong> or <strong>&ldquo;hdfs dfs&rdquo;</strong>, to be consistent in this exercise we will use &ldquo;hadoop fs&rdquo;</p>\n\n</div>"}]}},{"text":"%sh\n#hdfs dfs is the same as hadoop fs\nhadoop fs","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_358060388","id":"paragraph_1761427646257_1858697717","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#To begin with, we can see what is currently stored in HDFS by running the following: \"hadoop fs -ls\" or \"hdfs dfs -ls\". You might not get any output if you have not done GE2 on the terminal:\nhadoop fs -ls","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_1854094739","id":"paragraph_1761428119196_1916280542","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\n*Let’s create a folder in HDFS where we can store our own files: name the folder after yourself so that you can distinguish it from your group partner’s data. Note that <name> in the command below is a placeholder and should be replaced with your actual name.* Use:\n \"**hadoop fs -mkdir <name>**\" or \"**hdfs dfs -mkdir <name>**\" ","user":"anonymous","dateUpdated":"2025-10-26T20:42:21+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_744618637","id":"paragraph_1761428889196_1480854634","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:42:21+0000","dateStarted":"2025-10-26T20:42:21+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><em>Let’s create a folder in HDFS where we can store our own files: name the folder after yourself so that you can distinguish it from your group partner’s data. Note that &lt;name&gt; in the command below is a placeholder and should be replaced with your actual name.</em> Use:<br />\n&ldquo;<strong>hadoop fs -mkdir &lt;name&gt;</strong>&rdquo; or &ldquo;<strong>hdfs dfs -mkdir &lt;name&gt;</strong>&rdquo;</p>\n\n</div>"}]}},{"text":"%sh\nhadoop fs -mkdir <name>","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_1103064753","id":"paragraph_1761429044580_703799795","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\n*Run the hadoop fs -ls command again to verify that the folder was created in HDFS. Now we can copy the three text files from the local filesystem to our folder in HDFS by running the following command (again, replacing the <name> placeholder):* \n**hadoop fs -copyFromLocal file1, file2, file3 <name>/**","user":"anonymous","dateUpdated":"2025-10-26T20:42:26+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_1468468039","id":"paragraph_1761429456847_1557362533","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:42:26+0000","dateStarted":"2025-10-26T20:42:26+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><em>Run the hadoop fs -ls command again to verify that the folder was created in HDFS. Now we can copy the three text files from the local filesystem to our folder in HDFS by running the following command (again, replacing the &lt;name&gt; placeholder):</em><br />\n<strong>hadoop fs -copyFromLocal file1, file2, file3 &lt;name&gt;/</strong></p>\n\n</div>"}]}},{"text":"%sh\nhadoop fs -copyFromLocal GE2/Shakespeare_pp.txt GE2/Austen_pp.txt GE2/Churchill_pp.txt <name>/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_1467407094","id":"paragraph_1761430061675_1436202645","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#Verify that the files were copied to HDFS:\nhadoop fs -ls <name>/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_827188206","id":"paragraph_1761430338676_1607410742","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\n**3. Run Hadoop/MapReduce programs**\n*Now that we have our input data stored in HDFS, we can proceed to analyze the data using MapReduce. The file  hadoop-examples.jar, comprising Hadoop programs, including MapReduce implementations for calculating various word statistics, is also present in the folder GE2:* \nRun the following command to see which sample applications come with Hadoop: **hadoop jar GE2/hadoop-examples.jar**","user":"anonymous","dateUpdated":"2025-10-26T20:42:29+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006449_469411918","id":"paragraph_1761430417321_1215119934","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:42:29+0000","dateStarted":"2025-10-26T20:42:29+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>3. Run Hadoop/MapReduce programs</strong><br />\n<em>Now that we have our input data stored in HDFS, we can proceed to analyze the data using MapReduce. The file  hadoop-examples.jar, comprising Hadoop programs, including MapReduce implementations for calculating various word statistics, is also present in the folder GE2:</em><br />\nRun the following command to see which sample applications come with Hadoop: <strong>hadoop jar GE2/hadoop-examples.jar</strong></p>\n\n</div>"}]}},{"text":"%sh\nhadoop jar GE2/hadoop-examples.jar","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1602584510","id":"paragraph_1761430515022_2016221063","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\n*We are interested in running the final four MapReduce programs that calculate various word statistics:* **namely, wordcount, wordmean, wordmedian, wordstandarddeviation.** \n\n**3a. Word Count**\n*Run the following command to see how to use the WordCount application:* **hadoop jar GE2/hadoop-examples.jar wordcount.**\n","user":"anonymous","dateUpdated":"2025-10-26T20:43:11+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1351691594","id":"paragraph_1761430757753_1402141927","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:11+0000","dateStarted":"2025-10-26T20:43:11+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><em>We are interested in running the final four MapReduce programs that calculate various word statistics:</em> <strong>namely, wordcount, wordmean, wordmedian, wordstandarddeviation.</strong></p>\n<p><strong>3a. Word Count</strong><br />\n<em>Run the following command to see how to use the WordCount application:</em> <strong>hadoop jar GE2/hadoop-examples.jar wordcount.</strong></p>\n\n</div>"}]}},{"text":"%sh\nhadoop jar GE2/hadoop-examples.jar wordcount\n#The output of the command is \"Usage: wordcount <in> [<in>...] <out>\"","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1483646420","id":"paragraph_1761430867410_1430018199","dateCreated":"2025-10-26T20:20:06+0000","status":"READY","focus":false},{"text":"%md\nWe specify one or more files (Shakespeare_pp.txt, Austen_pp.txt, and Churchill_pp.txt) as input and an output directory. Run the WordCount application on each file separately; if you give all three files as input, the results will be merged, and this is not desirable in this case since we want to compare the three authors with respect to their word usage!\n\n*hadoop jar GE2/hadoop-examples.jar wordcount <name>/Shakespeare_pp.txt <name>/WordCount/Shakespeare/\nhadoop jar GE2/hadoop-examples.jar wordcount <name>/Austen_pp.txt <name>/WordCount/Austen/\nhadoop jar GE2/hadoop-examples.jar wordcount <name>/Churchill_pp.txt <name>/WordCount/Churchill/*\n","user":"anonymous","dateUpdated":"2025-10-26T20:43:20+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1218839065","id":"paragraph_1761431219966_184103605","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:20+0000","dateStarted":"2025-10-26T20:43:20+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We specify one or more files (Shakespeare_pp.txt, Austen_pp.txt, and Churchill_pp.txt) as input and an output directory. Run the WordCount application on each file separately; if you give all three files as input, the results will be merged, and this is not desirable in this case since we want to compare the three authors with respect to their word usage!</p>\n<p><em>hadoop jar GE2/hadoop-examples.jar wordcount &lt;name&gt;/Shakespeare_pp.txt &lt;name&gt;/WordCount/Shakespeare/<br />\nhadoop jar GE2/hadoop-examples.jar wordcount &lt;name&gt;/Austen_pp.txt &lt;name&gt;/WordCount/Austen/<br />\nhadoop jar GE2/hadoop-examples.jar wordcount &lt;name&gt;/Churchill_pp.txt &lt;name&gt;/WordCount/Churchill/</em></p>\n\n</div>"}]}},{"text":"%sh\n#replace <name> with the name you used earlier\nhadoop jar GE2/hadoop-examples.jar wordcount <name>/Shakespeare_pp.txt <name>/WordCount/Shakespeare/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1283686327","id":"paragraph_1761432004139_1342487379","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#replace <name> with the name you used earlier\nhadoop jar GE2/hadoop-examples.jar wordcount <name>/Austen_pp.txt <name>/WordCount/Austen/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1319086229","id":"paragraph_1761432125727_1318807477","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#replace <name> with the name you used earlier\nhadoop jar GE2/hadoop-examples.jar wordcount <name>/Churchill_pp.txt <name>/WordCount/Churchill/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_76019662","id":"paragraph_1761432185713_379746530","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\nAs the program is running, various information is shown on the screen, including progress made with respect to map and reduce phases:\n    - 2025-10-25 22:48:36,791 INFO mapreduce.Job:  map 0% reduce 0%\n    - 2025-10-25 22:48:40,827 INFO mapreduce.Job:  map 100% reduce 0%\n    - 2025-10-25 22:48:45,844 INFO mapreduce.Job:  map 100% reduce 100%\n\nWhen the application is done, both map and reduce will be at 100%. \nThe results are saved to the specified output folders: <name>/WordCount/<author>. Inspect the contents of these folders:\n\nRun **hadoop fs -ls <name>/WordCount/<name>**\n\nThe results are stored in files called **part-r-00000**. Later, we will copy these files to the local filesystem, inspect them, and anlyze ther results.","user":"anonymous","dateUpdated":"2025-10-26T20:43:28+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1277947025","id":"paragraph_1761432190802_360807332","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:28+0000","dateStarted":"2025-10-26T20:43:28+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>As the program is running, various information is shown on the screen, including progress made with respect to map and reduce phases:<br />\n- 2025-10-25 22:48:36,791 INFO mapreduce.Job:  map 0% reduce 0%<br />\n- 2025-10-25 22:48:40,827 INFO mapreduce.Job:  map 100% reduce 0%<br />\n- 2025-10-25 22:48:45,844 INFO mapreduce.Job:  map 100% reduce 100%</p>\n<p>When the application is done, both map and reduce will be at 100%.<br />\nThe results are saved to the specified output folders: &lt;name&gt;/WordCount/&lt;author&gt;. Inspect the contents of these folders:</p>\n<p>Run <strong>hadoop fs -ls &lt;name&gt;/WordCount/&lt;name&gt;</strong></p>\n<p>The results are stored in files called <strong>part-r-00000</strong>. Later, we will copy these files to the local filesystem, inspect them, and anlyze ther results.</p>\n\n</div>"}]}},{"text":"%md\n**3b. Word Mean**\n*Run the following command to see how to use the WordCount application:* **hadoop jar GE2/hadoop-examples.jar wordmean.**","user":"anonymous","dateUpdated":"2025-10-26T20:43:32+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1803374949","id":"paragraph_1761432819784_1849749437","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:32+0000","dateStarted":"2025-10-26T20:43:32+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>3b. Word Mean</strong><br />\n<em>Run the following command to see how to use the WordCount application:</em> <strong>hadoop jar GE2/hadoop-examples.jar wordmean.</strong></p>\n\n</div>"}]}},{"text":"%md\nWe run this program **three times**, once for each input file, similarly to the previous step:\n**mararow_dsv@dsvsandbox:~$** *hadoop jar GE2/hadoop-examples.jar wordmean <name>/*\n<input_file> <name>/WordMean/<author>/","user":"anonymous","dateUpdated":"2025-10-26T20:43:35+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1962043959","id":"paragraph_1761433414218_1224076712","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:35+0000","dateStarted":"2025-10-26T20:43:35+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We run this program <strong>three times</strong>, once for each input file, similarly to the previous step:<br />\n<strong>mararow_dsv@dsvsandbox:~$</strong> <em>hadoop jar GE2/hadoop-examples.jar wordmean &lt;name&gt;/</em><br />\n&lt;input_file&gt; &lt;name&gt;/WordMean/&lt;author&gt;/</p>\n\n</div>"}]}},{"text":"%sh\n#replace <name> with the name you used earlier\nhadoop jar GE2/hadoop-examples.jar wordmean eyasu/Shakespeare_pp.txt eyasu/WordMean/Shakespeare/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_600302920","id":"paragraph_1761433836934_1274071872","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#write the wordmean command for Austen","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_728523139","id":"paragraph_1761434062212_679759957","dateCreated":"2025-10-26T20:20:06+0000","status":"READY","focus":false},{"text":"%sh\n#write the wordmean command for Churchill","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_2052179532","id":"paragraph_1761434132370_1089136039","dateCreated":"2025-10-26T20:20:06+0000","status":"READY","focus":false},{"text":"%md\nThe result is printed on the screen at the end of each run **- write down the results!** The underlying data used for calculating the statistics are stored in the output files.\n\n**3c. Word Median**\nRun the following command to see how the Word Median application works:\n**mararow_dsv@dsvsandbox:~$** *hadoop jar GE2/hadoop-examples.jar wordmedian*\nUsage: wordmedian  <in> <out> \n\n\n\n","user":"anonymous","dateUpdated":"2025-10-26T20:43:40+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_694080843","id":"paragraph_1761434504458_1446330240","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:40+0000","dateStarted":"2025-10-26T20:43:40+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The result is printed on the screen at the end of each run <strong>- write down the results!</strong> The underlying data used for calculating the statistics are stored in the output files.</p>\n<p><strong>3c. Word Median</strong><br />\nRun the following command to see how the Word Median application works:<br />\n<strong>mararow_dsv@dsvsandbox:~$</strong> <em>hadoop jar GE2/hadoop-examples.jar wordmedian</em><br />\nUsage: wordmedian  &lt;in&gt; &lt;out&gt;</p>\n\n</div>"}]}},{"text":"%sh\nhadoop jar GE2/hadoop-examples.jar wordmedian","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1027974026","id":"paragraph_1761435518179_1117639304","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\nWe run this program three times, once for each input file, similarly to the previous step:\n**mararow_dsv@dsvsandbox:~$** *hadoop jar GE2/hadoop-examples.jar wordmean <name>/<input_file> <name>/WordMean/<author>/*\n\nRun the commands in the cells below. The result is printed on the screen at the end of each run **- write down the results!** The underlying data used for calculating the statistics are stored in the output files.","user":"anonymous","dateUpdated":"2025-10-26T20:43:45+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1254959132","id":"paragraph_1761434547779_486781643","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:45+0000","dateStarted":"2025-10-26T20:43:45+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We run this program three times, once for each input file, similarly to the previous step:<br />\n<strong>mararow_dsv@dsvsandbox:~$</strong> <em>hadoop jar GE2/hadoop-examples.jar wordmean &lt;name&gt;/&lt;input_file&gt; &lt;name&gt;/WordMean/&lt;author&gt;/</em></p>\n<p>Run the commands in the cells below. The result is printed on the screen at the end of each run <strong>- write down the results!</strong> The underlying data used for calculating the statistics are stored in the output files.</p>\n\n</div>"}]}},{"text":"%sh\n#replace <name> with the name you used earlier\nhadoop jar GE2/hadoop-examples.jar wordmedian <name>/Shakespeare_pp.txt <name>/WordMedian/Shakespeare/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1162722770","id":"paragraph_1761435033980_1248139359","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#write the wordmedian command for Austen","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_328622975","id":"paragraph_1761435042238_1695330467","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#write the wordmedian command for Churchill","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1814251581","id":"paragraph_1761435041521_1217482715","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\n**3d. Standard Deviation**\nRun the following command to see how the Standard Deviation application works:\n\n**mararow_dsv@dsvsandbox:~$** *hadoop jar GE2/hadoop-examples.jar wordstandarddeviation*\nUsage: wordstandarddeviation <in> <out>\n","user":"anonymous","dateUpdated":"2025-10-26T20:43:50+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_713861308","id":"paragraph_1761434785755_16056667","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:43:50+0000","dateStarted":"2025-10-26T20:43:50+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>3d. Standard Deviation</strong><br />\nRun the following command to see how the Standard Deviation application works:</p>\n<p><strong>mararow_dsv@dsvsandbox:~$</strong> <em>hadoop jar GE2/hadoop-examples.jar wordstandarddeviation</em><br />\nUsage: wordstandarddeviation &lt;in&gt; &lt;out&gt;</p>\n\n</div>"}]}},{"text":"%sh\nhadoop jar GE2/hadoop-examples.jar wordstandarddeviation","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1464441259","id":"paragraph_1761434790201_1414005904","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\nWe run this program three times, once for each input file, similarly to the previous steps:\n**mararow_dsv@dsvsandbox:~$** *hadoop jar GE2/hadoop-examples.jar wordstandarddeviation <name>/<input_file> <name>/WordStandardDeviation/<author>/*\n\n\nThe result is printed on the screen at the end of each run **- write down the results!** The underlying data used for calculating the statistics are stored in the output files.\n","user":"anonymous","dateUpdated":"2025-10-26T20:44:05+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1963098023","id":"paragraph_1761435590734_1993106901","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:44:05+0000","dateStarted":"2025-10-26T20:44:05+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We run this program three times, once for each input file, similarly to the previous steps:<br />\n<strong>mararow_dsv@dsvsandbox:~$</strong> <em>hadoop jar GE2/hadoop-examples.jar wordstandarddeviation &lt;name&gt;/&lt;input_file&gt; &lt;name&gt;/WordStandardDeviation/&lt;author&gt;/</em></p>\n<p>The result is printed on the screen at the end of each run <strong>- write down the results!</strong> The underlying data used for calculating the statistics are stored in the output files.</p>\n\n</div>"}]}},{"text":"%sh\n#replace <name> with the name you used earlier\nhadoop jar GE2/hadoop-examples.jar wordstandarddeviation <name>/Shakespeare_pp.txt <name>/WordStandardDeviation/Shakespeare/\n","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_201986374","id":"paragraph_1761436472178_383157283","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#write the wordstandarddeviation command for Austen","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_2010005691","id":"paragraph_1761436655330_1573116355","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#write the wordstandarddeviation command for Churchil\nhadoop jar GE2/hadoop-examples.jar wordstandarddeviation eyasu/Churchill_pp.txt eyasu/WordStandardDeviation/Churchill/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_725016041","id":"paragraph_1761436705721_913032070","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\n**4. Copy data from HDFS and analyze the results**\n\nNow that we have run our MapReduce applications to compute word statistics for three authors, let's copy the results to the local filesystem so that we can analyze them.\nFirst, create a folder (remember the folder you created in the previous steps is in the HDFS file system) with your name in the local file system to avoid interfering with your group partner's work. You can create this folder in the GE2 directory by using the following command\nmkdir GE2/<name>\ne.g., **mararow_dsv@dsvsandbox:~$** *mkdir GE2/<name>*","user":"anonymous","dateUpdated":"2025-10-26T20:44:10+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1045321206","id":"paragraph_1761436741735_1531265467","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:44:10+0000","dateStarted":"2025-10-26T20:44:10+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>4. Copy data from HDFS and analyze the results</strong></p>\n<p>Now that we have run our MapReduce applications to compute word statistics for three authors, let&rsquo;s copy the results to the local filesystem so that we can analyze them.<br />\nFirst, create a folder (remember the folder you created in the previous steps is in the HDFS file system) with your name in the local file system to avoid interfering with your group partner&rsquo;s work. You can create this folder in the GE2 directory by using the following command<br />\nmkdir GE2/&lt;name&gt;<br />\ne.g., <strong>mararow_dsv@dsvsandbox:~$</strong> <em>mkdir GE2/&lt;name&gt;</em></p>\n\n</div>"}]}},{"text":"%sh\n#mkdir GE2/<name>","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_807182253","id":"paragraph_1761437005591_267587527","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#To copy files from HDFS to the local filesystem, we can use the **hadoop fs -copyToLocal** command:\n#hadoop fs -copyToLocal <name>/W* GE2/<name>/\nhadoop fs -copyToLocal eyasu/W* GE2/eyasu/","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1200260643","id":"paragraph_1761437709453_478809563","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\nBy using the \"\\*\" wildcard, this command will copy all four output folders to the local filesystem (in the folder named after yourself). You can also choose to copy individual files or directories by specifying the full path.\n\nUse the ls command on the local filesystem to verify that the directories and files were copied as expected.\n\nTo open a results file from the Word Count (since these results were not displayed on the screen), you can use the less command, for example:\n\ne.g.,**mararow_dsv@dsvsandbox:~$** *less GE2/<name>/WordCount/Shakespeare/part-r-00000*\n","user":"anonymous","dateUpdated":"2025-10-26T20:44:14+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_177869495","id":"paragraph_1761437833070_820067647","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:44:14+0000","dateStarted":"2025-10-26T20:44:14+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>By using the &ldquo;*&rdquo; wildcard, this command will copy all four output folders to the local filesystem (in the folder named after yourself). You can also choose to copy individual files or directories by specifying the full path.</p>\n<p>Use the ls command on the local filesystem to verify that the directories and files were copied as expected.</p>\n<p>To open a results file from the Word Count (since these results were not displayed on the screen), you can use the less command, for example:</p>\n<p>e.g.,<strong>mararow_dsv@dsvsandbox:~$</strong> <em>less GE2/&lt;name&gt;/WordCount/Shakespeare/part-r-00000</em></p>\n\n</div>"}]}},{"text":"%sh\n#The command below will show you an ordered list of words and their frequency. \n#less GE2/<name>/WordCount/Shakespeare/part-r-00000 #this command lists a long list of word counts, so use the next one that limits to ten lines\nless GE2/<name>/WordCount/Shakespeare/part-r-00000| head -n 10","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_593708591","id":"paragraph_1761437893492_1470411591","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\nTo obtain a sorted list, you can run the sort command, The output is saved to the specified [output_file] and contains the words and their frequency sorted (descending) according to frequency:\n**mararow_dsv@dsvsandbox:~$** *sort -t $'\\t' -k2,2rn [wordcount input_file] > [output_file]*\n\n","user":"anonymous","dateUpdated":"2025-10-26T20:44:19+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1048665894","id":"paragraph_1761439052413_1258706941","dateCreated":"2025-10-26T20:20:06+0000","status":"FINISHED","dateFinished":"2025-10-26T20:44:19+0000","dateStarted":"2025-10-26T20:44:19+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>To obtain a sorted list, you can run the sort command, The output is saved to the specified [output_file] and contains the words and their frequency sorted (descending) according to frequency:<br />\n<strong>mararow_dsv@dsvsandbox:~$</strong> <em>sort -t $&lsquo;\\t&rsquo; -k2,2rn [wordcount input_file] &gt; [output_file]</em></p>\n\n</div>"}]}},{"text":"%sh\nsort -t $'\\t' -k2,2rn GE2/<name>/WordCount/Shakespeare/part-r-00000 > shakespearecount_yourname","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_527090703","id":"paragraph_1761439661341_40554561","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#create a sorted list for Austen","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1381815058","id":"paragraph_1761439794705_313324032","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#create a sorted list for Churchill","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1258882768","id":"paragraph_1761440014938_584979085","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%sh\n#To obtain a statistic for a specific word, you can use the grep command. For example:\ngrep \"misery\" GE2/<name>/WordCount/Shakespeare/part-r-00000","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_884080229","id":"paragraph_1761440142733_844473336","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"},{"text":"%md\nThat's all for this week's exercise! But you first need to answer a few questions before we finish (do your quiz)...\n","user":"anonymous","dateUpdated":"2025-10-26T20:20:06+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1761510006450_1882965694","id":"paragraph_1761440204622_1477792343","dateCreated":"2025-10-26T20:20:06+0000","status":"READY"}],"name":"Guided Exercise M2:Word Statistics with MapReduce","id":"2M7RRTXAX","defaultInterpreterGroup":"spark","version":"0.12.0","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{},"path":"/Guided Exercise M2:Word Statistics with MapReduce"}